{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions, InceptionV3\n",
    "from keras.models import Sequential\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, image_directory, \n",
    "                 batch_size=32, dim=(299, 299), n_channels=3,\n",
    "                 n_classes=128, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.image_directory = image_directory\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "         # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            img = image.load_img(\n",
    "                os.path.join(self.image_directory, 'pic_{}.png'.format(ID)), \n",
    "                target_size=self.dim)\n",
    "\n",
    "            # Store sample   \n",
    "            X[i,] =  preprocess_input(image.img_to_array(img))\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 128\n",
    "\n",
    "model = InceptionV3(weights='imagenet')\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[311].output)\n",
    "\n",
    "x = intermediate_layer_model.output\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "transfer_model = Model(inputs=intermediate_layer_model.input, outputs=predictions)\n",
    "\n",
    "# train last cluster and dense layer\n",
    "for layer in transfer_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# can train more \n",
    "for i in range(311,313):\n",
    "    transfer_model.layers[i].trainable = True\n",
    "\n",
    "transfer_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# using checkpoints and early stopping on validation sample to prevent overfitting\n",
    "# best weight is saved to file_path\n",
    "checkpoints_filepath=\"../checkpoints/weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(checkpoints_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "callbacks_list = [checkpoint, early] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_directory = '../data/train_images_readable/'\n",
    "validation_images_directory = '../data/validation_images_readable/'\n",
    "\n",
    "train_labels = ((pd\n",
    "    .read_csv('https://s3-us-west-2.amazonaws.com/furniture-kaggle/train-labels.csv')\n",
    "    .set_index('image_id')['label_id'] - 1)\n",
    "    .to_dict())\n",
    "\n",
    "validation_labels = ((pd\n",
    "    .read_csv('https://s3-us-west-2.amazonaws.com/furniture-kaggle/validation-labels.csv')\n",
    "    .set_index('image_id')['label_id'] - 1)\n",
    "    .to_dict())\n",
    "\n",
    "train_filenames = os.listdir(train_images_directory)\n",
    "train_ids = [filename.split('.')[0].split('_')[1] for filename in train_filenames]\n",
    "\n",
    "validation_filenames = os.listdir(validation_images_directory)\n",
    "validation_ids = [filename.split('.')[0].split('_')[1] for filename in validation_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_of_epochs = 20\n",
    "\n",
    "params = {'dim': (299, 299),\n",
    "          'batch_size': 32,\n",
    "          'n_classes': 128,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(train_ids, train_labels, train_images_directory, **params)\n",
    "validation_generator = DataGenerator(validation_ids, validation_labels, validation_images_directory, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    "    epochs=number_of_epochs,\n",
    "    use_multiprocessing=True,\n",
    "    max_queue_size=300,\n",
    "    workers=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "transfer_model.load_weights(checkpoints_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prediction on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_images_list = load_images('../data/test', (299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_array = np.array(\n",
    "    [image[1] \n",
    "     for image in test_images_list\n",
    "     if image[1].shape[2] == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions = [\n",
    "    [x[0], transfer_model.predict(x[1].reshape([1, 299, 299, 3]))] \n",
    "    for x in test_images_list \n",
    "    if (len(x[1].shape) == 3 and x[1].shape[2] == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_dict = dict([[int(x[0]), x[1]] for x in test_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_or_1(x):\n",
    "    return test_predictions_dict[x].argmax() + 1 if x in test_predictions_dict.keys() else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([[i, predict_or_1(i)] for i in range(1, 12801)], columns=['id', 'predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('first-submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
