{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import multiprocessing\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of functions used to sync images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image_id, url, output_directory, timeout):\n",
    "    output_path = '{}/pic_{}.png'.format(output_directory, image_id)\n",
    "        \n",
    "    try: \n",
    "        response = requests.get(url, timeout=timeout, stream=True)\n",
    "        \n",
    "        if response.status_code != requests.codes.OK:\n",
    "            raise Exception('Request exceeded {} seconds.'.format(timeout))\n",
    "        \n",
    "        with open(output_path, 'wb') as fh:\n",
    "            for chunk in response.iter_content(1024 * 1024):\n",
    "                fh.write(chunk)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print('{}: {}'.format(image_id, e))\n",
    "\n",
    "    except:\n",
    "        print('Unexpected error: {}'.format(sys.exc_info()[0]))\n",
    "        \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def download_image_list(image_ids, image_id_to_url, output_directory, pool_size, timeout):\n",
    "    pool = multiprocessing.Pool(pool_size)\n",
    "    \n",
    "    print(\"Attempting to download {} images in {}\".format(len(image_ids), output_directory))\n",
    "    \n",
    "    for image_id in image_ids:\n",
    "        pool.apply_async(download_image, [image_id, image_id_to_url[image_id], output_directory, timeout])\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filepath):\n",
    "    with open(filepath) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    return data\n",
    "\n",
    "def image_id_to_url(filepath):\n",
    "    data = read_json(filepath)\n",
    "    \n",
    "    return dict([\n",
    "        [link['image_id'], link['url'][0]] \n",
    "        for link in data['images']])\n",
    "    \n",
    "def images_in_directory(directory):\n",
    "    return set([\n",
    "        int(filename.split('_')[1].split('.')[0])\n",
    "        for filename in os.listdir(directory)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_images(id_to_url_filepath, output_directory, pool_size, timeout):\n",
    "    id_to_url = image_id_to_url(id_to_url_filepath)\n",
    "    ids = id_to_url.keys()\n",
    "    \n",
    "    available_images = images_in_directory(output_directory)\n",
    "\n",
    "    download_image_list(ids - available_images, id_to_url, output_directory, pool_size, timeout)\n",
    "    \n",
    "    return {image_id: id_to_url[image_id]\n",
    "            for image_id in (ids - available_images)}\n",
    "    \n",
    "def iterative_image_download(id_to_url_filepath, output_directory, missing_images_path, pool_size):\n",
    "    run_sync = lambda t: sync_images(id_to_url_filepath, output_directory, pool_size, t)\n",
    "    \n",
    "    unresolved_images = []\n",
    "    for tmo in (8*[1] + [5]):\n",
    "        unresolved_images = run_sync(tmo)\n",
    "    \n",
    "    with open(missing_images_path, 'w') as file:\n",
    "        json.dump(unresolved_images, file, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syncing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_url_filepath = '../data/id-to-url/{}.json'\n",
    "images_directory = '../data/{}_images/'\n",
    "missing_images_directory = '../data/missing_images_due_to_bad_download/'\n",
    "missing_images_filepath = os.path.join(missing_images_directory, 'missing_{}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['train', 'validation', 'test']:\n",
    "    !mkdir -p {images_directory.format(dataset)}\n",
    "    \n",
    "!mkdir -p {missing_images_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = 'train'\n",
    "\n",
    "iterative_image_download(\n",
    "    id_to_url_filepath.format(dataset),\n",
    "    images_directory.format(dataset),\n",
    "    missing_images_filepath.format(dataset),\n",
    "    500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = 'validation'\n",
    "\n",
    "iterative_image_download(\n",
    "    id_to_url_filepath.format(dataset),\n",
    "    images_directory.format(dataset),\n",
    "    missing_images_filepath.format(dataset),\n",
    "    500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = 'test'\n",
    "\n",
    "iterative_image_download(\n",
    "    id_to_url_filepath.format(dataset),\n",
    "    images_directory.format(dataset),\n",
    "    missing_images_filepath.format(dataset),\n",
    "    500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!zip -r ../data/train_images.zip ../data/train_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!zip -r ../data/validation_images.zip ../data/validation_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!zip -r ../data/train_images.zip ../data/train_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install aws-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ../data/train_images.zip s3://furniture-kaggle/\n",
    "!aws s3 cp ../data/validation_images.zip s3://furniture-kaggle/\n",
    "!aws s3 cp ../data/test_images.zip s3://furniture-kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ../data/missing_images_due_to_bad_download/missing_train.json s3://furniture-kaggle/\n",
    "!aws s3 cp ../data/missing_images_due_to_bad_download/missing_validation.json s3://furniture-kaggle/\n",
    "!aws s3 cp ../data/missing_images_due_to_bad_download/missing_test.json s3://furniture-kaggle/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_labels(filepath, outputpath):\n",
    "    data = read_json(filepath)\n",
    "        \n",
    "    annotations = data['annotations']\n",
    "    \n",
    "    pd.DataFrame(annotations).to_csv(outputpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "export_labels('../data/train.json', '../data/train-labels.csv')\n",
    "export_labels('../data/validation.json', '../data/validation-labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/train-labels.csv to s3://furniture-kaggle/train-labels.csv\n",
      "upload: ../data/validation-labels.csv to s3://furniture-kaggle/validation-labels.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ../data/train-labels.csv s3://furniture-kaggle/\n",
    "!aws s3 cp ../data/validation-labels.csv s3://furniture-kaggle/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
